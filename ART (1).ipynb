{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights initialized to:0.2, 0.2, 0.2, 0.2, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "\n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "\n",
      "Begin ART:\n",
      "Vector: 0\n",
      "\n",
      "InputSum (si) = 2\n",
      "\n",
      "0.2, 0.4, 0.4, 0.4, \n",
      "0.2, 0.4, 0.4, 0.4, \n",
      "0.2, 0.4, 0.4, 0.4, \n",
      "\n",
      "1 * 1.0 = 1.0\n",
      "1 * 1.0 = 1.0\n",
      "0 * 1.0 = 0.0\n",
      "0 * 1.0 = 0.0\n",
      "ActivationSum (x(i)) = 2\n",
      "\n",
      "0.6666666666666666, 0.6666666666666666, 0.0, 0.0, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "\n",
      "1, 1, 0, 0, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "\n",
      "Vector #0 belongs to cluster #0\n",
      "\n",
      "Vector: 1\n",
      "\n",
      "InputSum (si) = 1\n",
      "\n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.2, \n",
      "0.0, 0.0, 0.0, 0.2, \n",
      "\n",
      "0 * 1.0 = 0.0\n",
      "0 * 1.0 = 0.0\n",
      "0 * 1.0 = 0.0\n",
      "1 * 1.0 = 1.0\n",
      "ActivationSum (x(i)) = 1\n",
      "\n",
      "0.6666666666666666, 0.6666666666666666, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 1.0, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "\n",
      "1, 1, 0, 0, \n",
      "0, 0, 0, 1, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "\n",
      "Vector #1 belongs to cluster #1\n",
      "\n",
      "Vector: 2\n",
      "\n",
      "InputSum (si) = 1\n",
      "\n",
      "0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, \n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "\n",
      "1 * 1 = 1\n",
      "0 * 1 = 0\n",
      "0 * 0 = 0\n",
      "0 * 0 = 0\n",
      "ActivationSum (x(i)) = 1\n",
      "\n",
      "1.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 1.0, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "\n",
      "1, 0, 0, 0, \n",
      "0, 0, 0, 1, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "\n",
      "Vector #2 belongs to cluster #0\n",
      "\n",
      "Vector: 3\n",
      "\n",
      "InputSum (si) = 2\n",
      "\n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 1.0, \n",
      "0.0, 0.0, 0.2, 0.4, \n",
      "\n",
      "0 * 0 = 0\n",
      "0 * 0 = 0\n",
      "1 * 0 = 0\n",
      "1 * 1 = 1\n",
      "ActivationSum (x(i)) = 1\n",
      "\n",
      "1.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 1.0, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "\n",
      "1, 0, 0, 0, \n",
      "0, 0, 0, 1, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "\n",
      "Vector #3 belongs to cluster #1\n",
      "\n",
      "Vector: 4\n",
      "\n",
      "InputSum (si) = 1\n",
      "\n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.2, 0.2, 0.2, \n",
      "\n",
      "0 * 1.0 = 0.0\n",
      "1 * 1.0 = 1.0\n",
      "0 * 1.0 = 0.0\n",
      "0 * 1.0 = 0.0\n",
      "ActivationSum (x(i)) = 1\n",
      "\n",
      "Vector #4 belongs to cluster #2\n",
      "\n",
      "Vector: 5\n",
      "\n",
      "InputSum (si) = 1\n",
      "\n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.2, 0.2, \n",
      "\n",
      "0 * 1.0 = 0.0\n",
      "0 * 1.0 = 0.0\n",
      "1 * 1.0 = 1.0\n",
      "0 * 1.0 = 0.0\n",
      "ActivationSum (x(i)) = 1\n",
      "\n",
      "Vector #5 belongs to cluster #2\n",
      "\n",
      "Vector: 6\n",
      "\n",
      "InputSum (si) = 2\n",
      "\n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "0.0, 0.0, 0.0, 0.0, \n",
      "0.2, 0.2, 0.4, 0.4, \n",
      "\n",
      "1 * 1 = 1\n",
      "0 * 0 = 0\n",
      "1 * 0 = 0\n",
      "0 * 0 = 0\n",
      "ActivationSum (x(i)) = 1\n",
      "\n",
      "Vector #6 belongs to cluster #0\n",
      "\n",
      "Final weight values:\n",
      "1.0, 0.0, 0.0, 0.0, \n",
      "0.0, 0.0, 0.0, 1.0, \n",
      "0.2, 0.2, 0.2, 0.2, \n",
      "\n",
      "1, 0, 0, 0, \n",
      "0, 0, 0, 1, \n",
      "1.0, 1.0, 1.0, 1.0, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "VIGILANCE = 0.4\n",
    "PATTERNS = 7\n",
    "N = 4\n",
    "M = 3\n",
    "TRAINING_PATTERNS = 4 \n",
    "\n",
    "PATTERN_ARRAY = [[1, 1, 0, 0], \n",
    "                 [0, 0, 0, 1], \n",
    "                 [1, 0, 0, 0], \n",
    "                 [0, 0, 1, 1], \n",
    "                 [0, 1, 0, 0], \n",
    "                 [0, 0, 1, 0], \n",
    "                 [1, 0, 1, 0]]\n",
    "\n",
    "class ART:\n",
    "    def __init__(self, inputSize, numClusters, vigilance, numPatterns, numTraining, patternArray):\n",
    "        self.mInputSize = inputSize\n",
    "        self.mNumClusters = numClusters\n",
    "        self.mVigilance = vigilance\n",
    "        self.mNumPatterns = numPatterns\n",
    "        self.mNumTraining = numTraining\n",
    "        self.mPatterns = patternArray\n",
    "        \n",
    "        self.bw = [] # Bottom-up weights.\n",
    "        self.tw = [] # Top-down weights.\n",
    "\n",
    "        self.f1a = [] # Input layer.\n",
    "        self.f1b = [] # Interface layer.\n",
    "        self.f2 = []\n",
    "        return\n",
    "    \n",
    "    def initialize_arrays(self):\n",
    "        # Initialize bottom-up weight matrix.\n",
    "        sys.stdout.write(\"Weights initialized to:\")\n",
    "        for i in range(self.mNumClusters):\n",
    "            self.bw.append([0.0] * self.mInputSize)\n",
    "            for j in range(self.mInputSize):\n",
    "                self.bw[i][j] = 1.0 / (1.0 + self.mInputSize)\n",
    "                sys.stdout.write(str(self.bw[i][j]) + \", \")\n",
    "            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        # Initialize top-down weight matrix.\n",
    "        for i in range(self.mNumClusters):\n",
    "            self.tw.append([0.0] * self.mInputSize)\n",
    "            for j in range(self.mInputSize):\n",
    "                self.tw[i][j] = 1.0\n",
    "                sys.stdout.write(str(self.tw[i][j]) + \", \")\n",
    "            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        self.f1a = [0.0] * self.mInputSize\n",
    "        self.f1b = [0.0] * self.mInputSize\n",
    "        self.f2 = [0.0] * self.mNumClusters\n",
    "        return\n",
    "    \n",
    "    def get_vector_sum(self, nodeArray):\n",
    "        total = 0\n",
    "        length = len(nodeArray)\n",
    "        for i in range(length):\n",
    "            total += nodeArray[i]\n",
    "        \n",
    "        return total\n",
    "    \n",
    "    def get_maximum(self, nodeArray):\n",
    "        maximum = 0;\n",
    "        foundNewMaximum = False;\n",
    "        length = len(nodeArray)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            foundNewMaximum = False\n",
    "            for i in range(length):\n",
    "                if i != maximum:\n",
    "                    if nodeArray[i] > nodeArray[maximum]:\n",
    "                        maximum = i\n",
    "                        foundNewMaximum = True\n",
    "            \n",
    "            if foundNewMaximum == False:\n",
    "                done = True\n",
    "        \n",
    "        return maximum\n",
    "    \n",
    "    def test_for_reset(self, activationSum, inputSum, f2Max):\n",
    "        doReset = False\n",
    "        \n",
    "        if(float(activationSum) / float(inputSum) >= self.mVigilance):\n",
    "            doReset = False # Candidate is accepted.\n",
    "        else:\n",
    "            self.f2[f2Max] = -1.0 # Inhibit.\n",
    "            doReset = True # Candidate is rejected.\n",
    "        \n",
    "        return doReset\n",
    "    \n",
    "    def update_weights(self, activationSum, f2Max):\n",
    "        # Update bw(f2Max)\n",
    "        for i in range(self.mInputSize):\n",
    "            self.bw[f2Max][i] = (2.0 * float(self.f1b[i])) / (1.0 + float(activationSum))\n",
    "        \n",
    "        for i in range(self.mNumClusters):\n",
    "            for j in range(self.mInputSize):\n",
    "                sys.stdout.write(str(self.bw[i][j]) + \", \")\n",
    "            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        # Update tw(f2Max)\n",
    "        for i in range(self.mInputSize):\n",
    "            self.tw[f2Max][i] = self.f1b[i]\n",
    "        \n",
    "        for i in range(self.mNumClusters):\n",
    "            for j in range(self.mInputSize):\n",
    "                sys.stdout.write(str(self.tw[i][j]) + \", \")\n",
    "            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def ART(self):\n",
    "        inputSum = 0\n",
    "        activationSum = 0\n",
    "        f2Max = 0\n",
    "        reset = True\n",
    "        \n",
    "        sys.stdout.write(\"Begin ART:\\n\")\n",
    "        for k in range(self.mNumPatterns):\n",
    "            sys.stdout.write(\"Vector: \" + str(k) + \"\\n\\n\")\n",
    "            \n",
    "            # Initialize f2 layer activations to 0.0\n",
    "            for i in range(self.mNumClusters):\n",
    "                self.f2[i] = 0.0\n",
    "            \n",
    "            # Input pattern() to f1 layer.\n",
    "            for i in range(self.mInputSize):\n",
    "                self.f1a[i] = self.mPatterns[k][i]\n",
    "            \n",
    "            # Compute sum of input pattern.\n",
    "            inputSum = self.get_vector_sum(self.f1a)\n",
    "            sys.stdout.write(\"InputSum (si) = \" + str(inputSum) + \"\\n\\n\")\n",
    "            \n",
    "            # Compute activations for each node in the f1 layer.\n",
    "            # Send input signal from f1a to the fF1b layer.\n",
    "            for i in range(self.mInputSize):\n",
    "                self.f1b[i] = self.f1a[i]\n",
    "            \n",
    "            # Compute net input for each node in the f2 layer.\n",
    "            for i in range(self.mNumClusters):\n",
    "                for j in range(self.mInputSize):\n",
    "                    self.f2[i] += self.bw[i][j] * float(self.f1a[j])\n",
    "                    sys.stdout.write(str(self.f2[i]) + \", \")\n",
    "                \n",
    "                sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            \n",
    "            reset = True\n",
    "            while reset == True:\n",
    "                # Determine the largest value of the f2 nodes.\n",
    "                f2Max = self.get_maximum(self.f2)\n",
    "                \n",
    "                # Recompute the f1a to f1b activations (perform AND function)\n",
    "                for i in range(self.mInputSize):\n",
    "                    sys.stdout.write(str(self.f1b[i]) + \" * \" + str(self.tw[f2Max][i]) + \" = \" + str(self.f1b[i] * self.tw[f2Max][i]) + \"\\n\")\n",
    "                    self.f1b[i] = self.f1a[i] * math.floor(self.tw[f2Max][i])\n",
    "                \n",
    "                # Compute sum of input pattern.\n",
    "                activationSum = self.get_vector_sum(self.f1b)\n",
    "                sys.stdout.write(\"ActivationSum (x(i)) = \" + str(activationSum) + \"\\n\\n\")\n",
    "                \n",
    "                reset = self.test_for_reset(activationSum, inputSum, f2Max)\n",
    "            \n",
    "            # Only use number of TRAINING_PATTERNS for training, the rest are tests.\n",
    "            if k < self.mNumTraining:\n",
    "                self.update_weights(activationSum, f2Max)\n",
    "            \n",
    "            sys.stdout.write(\"Vector #\" + str(k) + \" belongs to cluster #\" + str(f2Max) + \"\\n\\n\")\n",
    "                \n",
    "        return\n",
    "    \n",
    "    def print_results(self):\n",
    "        sys.stdout.write(\"Final weight values:\\n\")\n",
    "        \n",
    "        for i in range(self.mNumClusters):\n",
    "            for j in range(self.mInputSize):\n",
    "                sys.stdout.write(str(self.bw[i][j]) + \", \")\n",
    "            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        for i in range(self.mNumClusters):\n",
    "            for j in range(self.mInputSize):\n",
    "                sys.stdout.write(str(self.tw[i][j]) + \", \")\n",
    "            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    art = ART(N, M, VIGILANCE, PATTERNS, TRAINING_PATTERNS, PATTERN_ARRAY)\n",
    "    art.initialize_arrays()\n",
    "    art.ART()\n",
    "    art.print_results()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Logging conf\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "class ART2(object):\n",
    "\n",
    "    def __init__(self, n=5, m=3, rho=0.9, theta=None):\n",
    "        \"\"\"\n",
    "        Create ART2 network with specified shape\n",
    "        For Input array I of size n, we need n input nodes in F1.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            feature dimension of input; number of nodes in F1\n",
    "        m : int\n",
    "            Number of neurons in F2 competition layer\n",
    "            max number of categories\n",
    "            compare to n_class\n",
    "        rho : float\n",
    "            Vigilance parameter\n",
    "            larger rho: less inclusive prototypes\n",
    "            smaller rho: more generalization\n",
    "        theta :\n",
    "            Suppression paramater\n",
    "        L : float\n",
    "            Learning parameter: # TODO\n",
    "        internal parameters\n",
    "        ----------\n",
    "        Bij: array of shape (m x n)\n",
    "            Feed-Forward weights\n",
    "        Tji: array of shape (n x m)\n",
    "            Feed-back weights\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_size = n\n",
    "        self.output_size = m\n",
    "\n",
    "        \"\"\"init layers\n",
    "        F0 --> F1 --> F2\n",
    "        S  --> X  --> Y\n",
    "        \"\"\"\n",
    "        # F2\n",
    "        self.yj = np.zeros(self.output_size)\n",
    "        self.active_cluster_units = []\n",
    "        # F1\n",
    "        self.xi = np.zeros(self.input_size)\n",
    "        # F0\n",
    "        self.si = np.zeros(self.input_size)\n",
    "\n",
    "        \"\"\"init parameters\"\"\"\n",
    "        self.params = {}\n",
    "        # a,b fixed weights in F1 layer; should not be zero\n",
    "        self.params['a'] = 10\n",
    "        self.params['b'] = 10\n",
    "        # c fixed weight used in testing for reset\n",
    "        self.params['c'] = 0.1\n",
    "        # d activation of winning F2 unit\n",
    "        self.params['d'] = 0.9\n",
    "        # c*d / (1-d)  must be less than or equal to one\n",
    "        # as ratio --> 1 for greater vigilance\n",
    "        self.params['e'] = 0.00001\n",
    "        # small param to prevent division by zero\n",
    "\n",
    "        # self.L = 2\n",
    "        # rho : vigilance parameter\n",
    "        self.rho = rho\n",
    "        # theta: noise suppression parameter\n",
    "        #   e.g. theta = 1 / sqrt(n)\n",
    "        if theta is None:\n",
    "            self.theta = 1 / np.sqrt(self.input_size)\n",
    "        else:\n",
    "            self.theta = theta\n",
    "        # alpha: learning rate. Small value : slower learning,\n",
    "        #  but also more likely to reach equilibrium in slow\n",
    "        # learning mode\n",
    "        self.alpha = 0.6\n",
    "\n",
    "        \"\"\"init weights\"\"\"\n",
    "        # Bij initially (7.0, 7.0) for each cluster unit\n",
    "        self.Bij = np.ones((n, m)) * 5.0\n",
    "        # Tji initially 0\n",
    "        self.Tji = np.zeros((m, n))\n",
    "\n",
    "        \"\"\"init other activations\"\"\"\n",
    "        self.ui = np.zeros(self.input_size)\n",
    "        self.vi = None\n",
    "\n",
    "        \"\"\"Other helpers\"\"\"\n",
    "        self.log = None\n",
    "\n",
    "    def compute(self, all_data):\n",
    "        \"\"\"Process and learn from all data\n",
    "        Step 1\n",
    "        fast learning: repeat this step until placement of\n",
    "        patterns on cluster units does not change\n",
    "        from one epoch to the next\n",
    "        \"\"\"\n",
    "        for iepoch in range(self.n_epochs):\n",
    "            self._training_epoch(all_data)\n",
    "\n",
    "            # test stopping condition for n_epochs\n",
    "\n",
    "        return True\n",
    "\n",
    "    def learning_trial(self, idata):\n",
    "        \"\"\"\n",
    "        Step 3-11\n",
    "        idata is a single row of input\n",
    "        A learning trial consists of one presentation of one input pattern.\n",
    "        V and P will reach equilibrium after two updates of F1\n",
    "        \"\"\"\n",
    "\n",
    "        self.log.info(\"Starting Learning Trial.\")\n",
    "        self.log.debug(\"input pattern: {}\".format(idata))\n",
    "        self.log.debug(\"theta: {}\".format(self.theta))\n",
    "\n",
    "        # at beginning of learning trial, set all activations to zero\n",
    "        self._zero_activations()\n",
    "\n",
    "        self.si = idata\n",
    "        # TODO: Should this be here?\n",
    "\n",
    "        # Update F1 activations, no candidate cluster unit\n",
    "        self._update_F1_activation()\n",
    "\n",
    "        # Update F1 activations again\n",
    "        self._update_F1_activation()\n",
    "\n",
    "        \"\"\"\n",
    "        After F1 activations achieve equilibrium\n",
    "        TMP: Assume only two F1 updates needed for now\n",
    "        Then proceed feed-forward to F2\n",
    "        \"\"\"\n",
    "        # TODO: instead check if ui or pi will change significantly\n",
    "\n",
    "        # now P units send signals to F2 layer\n",
    "        self.yj = np.dot(self.Bij.T, self.pi)\n",
    "\n",
    "        J = self._select_candidate_cluster_unit()\n",
    "\n",
    "        \"\"\"step 8 (resonance)\n",
    "        reset cannot occur during resonance\n",
    "        new winning unit (J) cannot be chosen during resonance\n",
    "        \"\"\"\n",
    "        if len(self.active_cluster_units) == 0:\n",
    "            self._update_weights_first_pattern(J)\n",
    "        else:\n",
    "            self._resonance_learning(J)\n",
    "\n",
    "        # add J to active list\n",
    "        if J not in self.active_cluster_units:\n",
    "            self.active_cluster_units.append(J)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _training_epoch(self, all_data):\n",
    "\n",
    "        # initialize parameters and weights\n",
    "        pass  # done in __init__\n",
    "\n",
    "        for idata in all_data:\n",
    "            self.si = idata  # input vector F0\n",
    "\n",
    "            self.learning_trial()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _select_candidate_cluster_unit(self):\n",
    "        \"\"\" RESET LOOP\n",
    "        This loop selects an appropriate candidate cluster unit for learninig\n",
    "         - Each iteration selects a candidate unit.\n",
    "         - Iterations continue until reset condition is met (reset is False)\n",
    "         - if a candidate unit does not satisfy, it is inhibited and can not be\n",
    "         selected again in this presentation of the input pattern.\n",
    "        No learning occurs in this phase.\n",
    "        returns:\n",
    "            J, the index of the selected cluster unit\n",
    "        \"\"\"\n",
    "        self.reset = True\n",
    "        while self.reset:\n",
    "            self.log.info(\"candidate selection loop iter start\")\n",
    "            #  check reset\n",
    "\n",
    "            # Select best active candidate\n",
    "            # ... largest element of Y that is not inhibited\n",
    "            J = np.argmax(self.yj)  # J current candidate, not same as index jj\n",
    "\n",
    "            self.log.debug(\"\\tyj: {}\".format(self.yj))\n",
    "            self.log.debug(\"\\tpicking J = {}\".format(J))\n",
    "            # Test stopping condition here\n",
    "            # (check reset)\n",
    "\n",
    "            e = self.params['e']\n",
    "\n",
    "            #  confirm candidate: inhibit or proceed\n",
    "            if (self.vi == 0).all():\n",
    "                self.ui = np.zeros(self.input_size)\n",
    "            else:\n",
    "                self.ui = self.vi / (e + norm(self.vi))\n",
    "            # pi =\n",
    "\n",
    "            # calculate ri (reset node)\n",
    "            c = self.params['c']\n",
    "            term1 = norm(self.ui + c*self.ui)\n",
    "            term2 = norm(self.ui) + c*norm(self.ui)\n",
    "            self.ri = term1 / term2\n",
    "\n",
    "            if self.ri >= (self.rho - e):\n",
    "                self.log.info(\"\\tReset is False: Candidate is good.\")\n",
    "                # Reset condition satisfied: cluster unit may learn\n",
    "                self.reset = False\n",
    "\n",
    "                # finish updating F1 activations\n",
    "                self._update_F1_activation()\n",
    "                # TODO: this will update ui twice. Confirm ok\n",
    "            elif self.ri < (self.rho - e):\n",
    "                self.reset = True\n",
    "                self.log.info(\"\\treset is True\")\n",
    "                self.yj[J] = -1.0\n",
    "\n",
    "            # break inf loop manually\n",
    "            # self.log.warn(\"EXIT RESET LOOP MANUALLY\")\n",
    "            # self.reset = False\n",
    "\n",
    "        return J\n",
    "\n",
    "    def _resonance_learning(self, J, n_iter=20):\n",
    "        \"\"\"\n",
    "        Learn on confirmed candidate\n",
    "        In slow learning, only one update of weights in this trial\n",
    "            n_learning_iterations = 1\n",
    "            we then present the next input pattern\n",
    "        In fast learning, present input again (same learning trial)\n",
    "          - until weights reach equilibrium for this trial\n",
    "          - presentation is: \"weight-update-F1-update\"\n",
    "        \"\"\"\n",
    "        self.log.info(\"Entering Resonance phase with J = {}\".format(J))\n",
    "\n",
    "        for ilearn in range(n_iter):\n",
    "            self.log.info(\"learning iter start\")\n",
    "\n",
    "            self._update_weights(J)\n",
    "\n",
    "            # in slow learning, this step not required?\n",
    "            D = np.ones(self.output_size)\n",
    "            self._update_F1_activation(J, D)\n",
    "\n",
    "            # test stopping condition for weight updates\n",
    "            # if change in weights was below some tolerance\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _update_weights_first_pattern(self, J):\n",
    "        \"\"\"Equilibrium weights for the first pattern presented\n",
    "        converge to these values. This shortcut can save many\n",
    "        iterations.\n",
    "        \"\"\"\n",
    "        self.log.info(\"Weight update using first-pattern shortcut\")\n",
    "        # Fast learning first pattern simplification\n",
    "        d = self.params['d']\n",
    "        self.Tji[J, :] = self.ui / (1 - d)\n",
    "        self.Bij[:, J] = self.ui / (1 - d)\n",
    "\n",
    "        # log\n",
    "        self.log.debug(\"Tji[J]: {}\".format(self.Tji[J, :]))\n",
    "        self.log.debug(\"Bij[J]: {}\".format(self.Bij[:, J]))\n",
    "\n",
    "        return\n",
    "\n",
    "    def _update_weights(self, J):\n",
    "        \"\"\"update weights\n",
    "        for Tji and Bij\n",
    "        \"\"\"\n",
    "        self.log.info(\"Updating Weights\")\n",
    "\n",
    "        # get useful terms\n",
    "        alpha = self.alpha\n",
    "        d = self.params['d']\n",
    "\n",
    "        term1 = alpha*d*self.ui\n",
    "        term2 = (1 + alpha*d*(d - 1))\n",
    "\n",
    "        self.Tji[J, :] = term1 + term2*self.Tji[J, :]\n",
    "        self.Bij[:, J] = term1 + term2*self.Bij[:, J]\n",
    "\n",
    "        # log\n",
    "        self.log.debug(\"Tji[J]: {}\".format(self.Tji[J, :]))\n",
    "        self.log.debug(\"Bij[J]: {}\".format(self.Bij[:, J]))\n",
    "\n",
    "        return\n",
    "\n",
    "    def _update_F1_activation(self, J=None, D=None):\n",
    "        \"\"\"\n",
    "        if winning unit has been selected\n",
    "          J is winning cluster unit\n",
    "          D is F2 activation\n",
    "        else if no winning unit selected\n",
    "          J is None\n",
    "          D is zero vector\n",
    "        \"\"\"\n",
    "        # Checks\n",
    "        # self.log.warn(\"Warning: Skipping J xor D check!\")\n",
    "        # if (J is None) ^ (D is None):\n",
    "        #     raise Exception(\"Must provide both J and D, or neither.\")\n",
    "\n",
    "        msg = \"Updating F1 activations\"\n",
    "        if J is not None:\n",
    "            msg = msg + \" with J = {}\".format(J)\n",
    "        self.log.info(msg)\n",
    "\n",
    "        a = self.params['a']\n",
    "        b = self.params['b']\n",
    "        d = self.params['d']\n",
    "        e = self.params['e']\n",
    "\n",
    "        # compute activation of Unit Ui\n",
    "        #  - activation of Vi normalized to unit length\n",
    "        if self.vi is None:\n",
    "            self.ui = np.zeros(self.input_size)\n",
    "        else:\n",
    "            self.ui = self.vi / (e + norm(self.vi))\n",
    "\n",
    "        # signal sent from each unit Ui to associated Wi and Pi\n",
    "\n",
    "        # compute activation of Wi\n",
    "\n",
    "        self.wi = self.si + a * self.ui\n",
    "        # compute activation of pi\n",
    "        # WRONG: self.pi = self.ui + np.dot(self.yj, self.Tji)\n",
    "        if J is not None:\n",
    "            self.pi = self.ui + d * self.Tji[J, :]\n",
    "        else:\n",
    "            self.pi = self.ui\n",
    "\n",
    "        # TODO: consider RESET here\n",
    "\n",
    "        # compute activation of Xi\n",
    "        # self.xi = self._thresh(self.wi / norm(self.wi))\n",
    "        self.xi = self.wi / (e + norm(self.wi))\n",
    "        # compute activation of Qi\n",
    "        # self.qi = self._thresh(self.pi / (e + norm(self.pi)))\n",
    "        self.qi = self.pi / (e + norm(self.pi))\n",
    "\n",
    "        # send signal to Vi\n",
    "        self.vi = self._thresh(self.xi) + b * self._thresh(self.qi)\n",
    "\n",
    "        self._log_values()\n",
    "        return True\n",
    "\n",
    "    \"\"\"Helper methods\"\"\"\n",
    "    def _zero_activations(self):\n",
    "        \"\"\"Set activations to zero\n",
    "        common operation, e.g. beginning of a learning trial\n",
    "        \"\"\"\n",
    "        self.log.debug(\"zero'ing activations\")\n",
    "        self.si = np.zeros(self.input_size)\n",
    "        self.ui = np.zeros(self.input_size)\n",
    "        self.vi = np.zeros(self.input_size)\n",
    "        return\n",
    "\n",
    "    def _thresh(self, vec):\n",
    "        \"\"\"\n",
    "        This function treats any signal that is less than theta\n",
    "        as noise and suppresses it (sets it to zero). The value\n",
    "        of the parameter theta is specified by the user.\n",
    "        \"\"\"\n",
    "        assert isinstance(vec, np.ndarray), \"type check\"\n",
    "        cpy = vec.copy()\n",
    "        cpy[cpy < self.theta] = 0\n",
    "        return cpy\n",
    "\n",
    "    def _clean_input_pattern(self, idata):\n",
    "        assert len(idata) == self.input_size, \"size check\"\n",
    "        assert isinstance(idata, np.ndarray), \"type check\"\n",
    "\n",
    "        return idata\n",
    "\n",
    "    \"\"\"Logging Functions\"\"\"\n",
    "    def stop_logging(self):\n",
    "        \"\"\"Logging stuff\n",
    "        closes filehandlers and stuff\n",
    "        \"\"\"\n",
    "        self.log.info('Stop Logging.')\n",
    "        handlers = self.log.handlers[:]\n",
    "        for handler in handlers:\n",
    "            handler.close()\n",
    "            self.log.removeHandler(handler)\n",
    "        self.log = None\n",
    "\n",
    "    def start_logging(self, to_file=True, to_console=True):\n",
    "        \"\"\"Logging!\n",
    "        init logging handlers and stuff\n",
    "        to_file and to_console are booleans\n",
    "        # TODO: accept logging level\n",
    "        \"\"\"\n",
    "        # remove any existing logger\n",
    "        if self.log is not None:\n",
    "            self.stop_logging()\n",
    "            self.log = None\n",
    "\n",
    "        # Create logger and configure\n",
    "        self.log = logging.getLogger('ann.art.art2')\n",
    "        self.log.setLevel(logging.DEBUG)\n",
    "        self.log.propagate = False\n",
    "        formatter = logging.Formatter(\n",
    "            fmt='%(levelname)8s:%(message)s'\n",
    "        )\n",
    "\n",
    "        # add file logging\n",
    "        if to_file:\n",
    "            fh = logging.FileHandler(\n",
    "                filename='ART_LOG.log',\n",
    "                mode='w',\n",
    "            )\n",
    "            fh.setFormatter(formatter)\n",
    "            fh.setLevel(logging.WARN)\n",
    "            self.log.addHandler(fh)\n",
    "\n",
    "        # create console handler with a lower log level for debugging\n",
    "        if to_console:\n",
    "            ch = logging.StreamHandler(sys.stdout)\n",
    "            ch.setFormatter(formatter)\n",
    "            ch.setLevel(logging.DEBUG)\n",
    "            self.log.addHandler(ch)\n",
    "\n",
    "        self.log.info('Start Logging')\n",
    "\n",
    "    def getlogger(self):\n",
    "        \"\"\"Logging stuff\n",
    "        \"\"\"\n",
    "        return self.log\n",
    "\n",
    "    def _log_values(self, J=None):\n",
    "        \"\"\"Logging stuff\n",
    "        convenience function\n",
    "        \"\"\"\n",
    "        self.log.debug(\"\\t--- debug values --- \")\n",
    "        self.log.debug(\"\\tui : {}\".format(self.ui))\n",
    "        self.log.debug(\"\\twi : {}\".format(self.wi))\n",
    "        self.log.debug(\"\\tpi : {}\".format(self.pi))\n",
    "        self.log.debug(\"\\txi : {}\".format(self.xi))\n",
    "        self.log.debug(\"\\tqi : {}\".format(self.qi))\n",
    "        self.log.debug(\"\\tvi : {}\".format(self.vi))\n",
    "        if J is not None:\n",
    "            self.log.debug(\"\\tWeights with J = {}\".format(J))\n",
    "            self.log.debug(\"\\tBij: {}\".format(self.bij[:, J]))\n",
    "            self.log.debug(\"\\tTji: {}\".format(self.tji[J, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
